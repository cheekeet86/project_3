{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Web APIs & Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "- [Problem Statement](#Problem-Statement)\n",
    "- [Functions](#Functions)\n",
    "- [Importing](#Importing)\n",
    "\n",
    "\n",
    "- [Inspect Data](#Inspect-Data)\n",
    "- [Clean Data](#Clean-Data)\n",
    "- [Output Clean Data](#Output-Clean-Data)\n",
    "\n",
    "\n",
    "- [EDA](#EDA)\n",
    "\n",
    "\n",
    "- [Create Feature Matrix and Target](#Create-Feature-Matrix-and-Target)\n",
    "\n",
    "\n",
    "\n",
    "- [Output Model Predictions](#Output-Model-Predictions)\n",
    "\n",
    "\n",
    "- [Descriptive and Inferential Statistics](#Descriptive-and-Inferential-Statistics)\n",
    "- [Outside Research](#Outside-Research)\n",
    "- [Conclusions and Recommendations](#Conclusions-and-Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "- Blank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user configuration\n",
    "\n",
    "# True: scape data from website -> save as json files\n",
    "# False: load json files\n",
    "scape_data = False\n",
    "\n",
    "# scape data for subreddits[0] if scape_data=True\n",
    "#scape_index = 0\n",
    "# scape data for subreddits[1] if scape_data=True\n",
    "scape_index = 1\n",
    "\n",
    "subreddits = ['boardgames','mobilegames']\n",
    "\n",
    "url = \"http://www.reddit.com/r/\"\n",
    "headers = {'User-agent':'Bleep blorp bot 0.1'}\n",
    "\n",
    "num_requests = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "# maths\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "# visual\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# modelling\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler,PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "\n",
    "# nlp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# web\n",
    "import requests\n",
    "import json\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# others\n",
    "import time\n",
    "import datetime as dt\n",
    "#import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "\n",
    "input_path = '../data/input/'\n",
    "mid_path = '../data/mid/'\n",
    "output_path = '../data/output/'\n",
    "\n",
    "image_path = '../images/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output scaped data to json files\n",
    "def write_json_files(data):    \n",
    "    \n",
    "    timestamp = dt.datetime.now()\n",
    "    timestamp = timestamp.strftime(\" %Y_%m_%d %H_%M_%S\")\n",
    "    \n",
    "    filepath = input_path + subreddits[scape_index] + '/'\n",
    "    filename = subreddits[scape_index] + timestamp + '.json'\n",
    "    \n",
    "    if not os.path.exists(filepath):\n",
    "        os.makedirs(filepath)\n",
    "    \n",
    "    file = open(filepath + filename,\"w+\")    \n",
    "    json.dump(data,file)\n",
    "    \n",
    "    print('created',filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create posts list from json files\n",
    "def read_json_files(subreddit):   \n",
    "    \n",
    "    files = []    \n",
    "    filepath = input_path + subreddit\n",
    "\n",
    "    # r=root, d=directories, f = files\n",
    "    for r, d, f in os.walk(filepath):\n",
    "        for file in f:\n",
    "            if '.json' in file:\n",
    "                files.append(os.path.join(r, file))\n",
    "                \n",
    "    posts = []\n",
    "\n",
    "    for f in files:\n",
    "        print(f)\n",
    "        file_data = open(f).read()\n",
    "        json_data = json.loads(file_data)\n",
    "\n",
    "        posts.extend(json_data['data']['children'])\n",
    "        after = json_data['data']['after']\n",
    "        \n",
    "    return posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scape or Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old method (not used)\n",
    "# get json from reddit boardgames\n",
    "\n",
    "# if scape_data == True:\n",
    "\n",
    "#     response = requests.get(url,headers=headers)\n",
    "#     print(response.status_code)\n",
    "\n",
    "#     json_data = response.json()\n",
    "#     sorted(json_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old method (not used)\n",
    "# print number of posts in json_data\n",
    "\n",
    "# if scape_data == True:\n",
    "\n",
    "#     print(len(json_data['data']['children']))\n",
    "\n",
    "#     # print info in kind\n",
    "#     print(json_data['kind'])\n",
    "#     print('')\n",
    "\n",
    "#     # print data keys\n",
    "#     print(sorted(json_data['data'].keys()))\n",
    "#     print('')\n",
    "\n",
    "#     # print 1st post\n",
    "#     print(json_data['data']['children'][0]['data'])\n",
    "#     print('')\n",
    "\n",
    "#     # print id of of last post\n",
    "#     print(json_data['data']['after'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old method (not used)\n",
    "# print ids of all posts\n",
    "\n",
    "# if scape_data == True:\n",
    "\n",
    "#     [post['data']['name'] for post in json_data['data']['children']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get json_data with multiple requests\n",
    "\n",
    "if scape_data == True:\n",
    "\n",
    "    posts = []\n",
    "    after = None\n",
    "\n",
    "    for i in range(1,num_requests+1):\n",
    "\n",
    "        if after == None:\n",
    "            params = {}\n",
    "        else:\n",
    "            params = {'after':after}\n",
    "\n",
    "        json_url = url + subreddits[scape_index] + '.json'\n",
    "        response = requests.get(json_url,params=params,headers=headers)\n",
    "\n",
    "        if response.status_code == 200:       \n",
    "\n",
    "            print('process request',i)\n",
    "\n",
    "            json_data = response.json()\n",
    "            posts.extend(json_data['data']['children'])\n",
    "            after = json_data['data']['after']\n",
    "\n",
    "            # save scaped data to files\n",
    "            write_json_files(json_data)        \n",
    "\n",
    "        else:\n",
    "            print(response.status_code)\n",
    "            break\n",
    "\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json data from files (saved previously in input folder)\n",
    "\n",
    "if scape_data == False:\n",
    "    \n",
    "    posts_a = read_json_files(subreddits[0])\n",
    "    posts_b = read_json_files(subreddits[1])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posts count info\n",
    "\n",
    "if scape_data == False:\n",
    "    \n",
    "    print('total number of posts:',len(posts_a))\n",
    "    print('unique number of posts:',len(set(p['data']['name'] for p in posts_a)))\n",
    "    print('')\n",
    "    print('total number of posts:',len(posts_b))\n",
    "    print('unique number of posts:',len(set(p['data']['name'] for p in posts_b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only inspect posts from subreddit[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some info from posts_a (subreddit[0])\n",
    "\n",
    "# for p in posts_a:\n",
    "    \n",
    "#     title = p['data']['title']\n",
    "#     #subreddit = p['data']['subreddit']\n",
    "#     score = p['data']['score']\n",
    "#     pinned = p['data']['pinned'] \n",
    "    \n",
    "#     print(title)\n",
    "#     print(subreddit)\n",
    "#     print(score)\n",
    "#     print(pinned)\n",
    "#     print('')\n",
    "    \n",
    "#     keys = p['data'].keys()    \n",
    "#     print(len(keys))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty df\n",
    "\n",
    "#cols = list(posts_a[0]['data'].keys())\n",
    "cols = ['title','score','selftext']\n",
    "\n",
    "df = pd.DataFrame(columns=cols)\n",
    "df_a = pd.DataFrame(columns=cols)\n",
    "df_b = pd.DataFrame(columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert posts -> df\n",
    "\n",
    "for idx,p in enumerate(posts_a):\n",
    "    \n",
    "    p_keys = p['data'].keys()\n",
    "    \n",
    "    for col in cols:\n",
    "        \n",
    "        if col in p_keys:\n",
    "            df_a.at[idx,col] = p['data'][col]\n",
    "        else:\n",
    "            df_a.at[idx,col] = np.nan\n",
    "            print(idx,col,'nan')\n",
    "            \n",
    "for idx,p in enumerate(posts_b):\n",
    "    \n",
    "    p_keys = p['data'].keys()\n",
    "    \n",
    "    for col in cols:\n",
    "        \n",
    "        if col in p_keys:\n",
    "            df_b.at[idx,col] = p['data'][col]\n",
    "        else:\n",
    "            df_b.at[idx,col] = np.nan\n",
    "            print(idx,col,'nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all columns\n",
    "\n",
    "print(df_a.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output 1st 5 records in df_train\n",
    "\n",
    "df_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a_desc = df_a.describe()\n",
    "df_a_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nulls in columns\n",
    "\n",
    "null_cols = df_a.isnull().sum()\n",
    "mask_null = null_cols > 0\n",
    "null_cols[mask_null].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for nulls in rows\n",
    "\n",
    "null_rows = df_a.isnull().sum(axis=1)\n",
    "mask_null = null_rows > 0\n",
    "null_rows[mask_null].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show column summary\n",
    "\n",
    "df_a_info = df_a.info()\n",
    "df_a_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find columns with the most empty cells\n",
    "\n",
    "df_a.count(axis=0).sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all string in cells to lowercase -> prevent duplicates when creating dummies\n",
    "\n",
    "#df_train = df_train.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "#df_test = df_test.applymap(lambda s:s.lower() if type(s) == str else s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nan/empty cells with na\n",
    "\n",
    "#cols = ['pool_qual','bsmt_qual','fireplace_score','garage_qual','garage_cond','bsmt_type1_score','bsmt_type2_score']\n",
    "\n",
    "#for col in cols:\n",
    "    #df_train[col] = df_train[col].fillna(value='na')\n",
    "    #df_test[col] = df_test[col].fillna(value='na')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # fill nan/empty cells with 0\n",
    "\n",
    "# cols = ['bath_half_bsmt_num','bath_full_bsmt_num','garage_area','garage_car_num','bsmt_total_area',\n",
    "#         'bsmt_unfinish_area','bsmt_type2_area','bsmt_type1_area']\n",
    "\n",
    "# for col in cols:\n",
    "#     df_train[col] = df_train[col].fillna(value='0')\n",
    "#     df_test[col] = df_test[col].fillna(value='0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to csv\n",
    "\n",
    "#df_a.to_csv(mid_path + 'df_a.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train heatmap (staircase)\n",
    "\n",
    "#corr = df_train.corr()\n",
    "#mask = np.zeros_like(corr)\n",
    "#mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "#fig, ax = plt.subplots(figsize=(20, 10))\n",
    "#sns.heatmap(corr, mask=mask, vmax=.3, square=True,cmap=\"coolwarm_r\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_scatterplot('Sale Price vs House Quality',df_train,x ='house_qual',y ='sale_price',hue ='lot_subclass')\n",
    "\n",
    "#print(\"sale price tends to increase as house quality increases.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create histograms for all numeric columns\n",
    "#df_train.hist(figsize=(15, 15));\n",
    "\n",
    "#print('plot histograms for all numeric columns to check for zeros and abnormalities.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_boxplot(df_train,x='lot_subclass',y='sale_price',title='Sale Price vs Lot Subclass')\n",
    "\n",
    "#print(\"Most subclasses has sale price between 100K to 200K.\")\n",
    "#print(\"4 subclasses have sale price above 200k and they have more outliers.\")\n",
    "#print(\"I will convert lot_subclass to dummy variables for model predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use post title for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_a.shape)\n",
    "print(df_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(df_a)\n",
    "df = df.append(df_b)\n",
    "\n",
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target (use one hot encoding)\n",
    "df['is_a'].iloc[0:len(df_a)] = 1\n",
    "df['is_a'].iloc[len(df_a):] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X (feature matrix) and y (target)\n",
    "feature_cols = ['title']\n",
    "X = df[feature_cols]\n",
    "y = df['is_a'].values\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.7,test_size=0.3,stratify=y,random_state=3050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check train and test data\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use CountVectorizer\n",
    "cvec = CountVectorizer(stop_words='english')\n",
    "cvec.fit(X_train['title'])\n",
    "\n",
    "len_features = len(cvec.get_feature_names())\n",
    "print(len_features)\n",
    "\n",
    "X_train_cv = pd.DataFrame(cvec.transform(X_train['title']).todense(),columns=cvec.get_feature_names())\n",
    "X_test_cv = pd.DataFrame(cvec.transform(X_test['title']).todense(),columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_cv,y_train)\n",
    "\n",
    "lr_score = lr.score(X_test_cv, y_test)\n",
    "print(lr_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output to csv\n",
    "\n",
    "#timestamp = dt.datetime.now()\n",
    "#timestamp = timestamp.strftime(\" %Y_%m_%d %H_%M_%S \")\n",
    "    \n",
    "# contains selected columns for feature matrix\n",
    "#df_cols.to_csv(output_path + 'columns' + timestamp + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive and Inferential Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outside Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
